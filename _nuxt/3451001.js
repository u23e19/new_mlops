(window.webpackJsonp=window.webpackJsonp||[]).push([[2],{133:function(n,e,t){"use strict";var o=t(0),r=t(199);e.a=function(n){var e=n.app;o.a.use(r.a,{config:{id:"G-8TS3X7Y01N"},appName:"MLOps Toys",pageTrackerScreenviewEnabled:!0},e.router),o.a.use({install:function(n){n.prototype.$gtagEvents={externalLinkClick:function(e){n.$gtag.event("click",{event_category:"outbound",event_label:e,transport_type:"beacon"})}}}})}},203:function(n,e,t){"use strict";t(42),t(32),t(33),t(56),t(38),t(57);var o=t(20),r=t(45),l=t(194),d=t(202);function c(object,n){var e=Object.keys(object);if(Object.getOwnPropertySymbols){var t=Object.getOwnPropertySymbols(object);n&&(t=t.filter((function(n){return Object.getOwnPropertyDescriptor(object,n).enumerable}))),e.push.apply(e,t)}return e}function m(n){for(var i=1;i<arguments.length;i++){var source=null!=arguments[i]?arguments[i]:{};i%2?c(Object(source),!0).forEach((function(e){Object(o.a)(n,e,source[e])})):Object.getOwnPropertyDescriptors?Object.defineProperties(n,Object.getOwnPropertyDescriptors(source)):c(Object(source)).forEach((function(e){Object.defineProperty(n,e,Object.getOwnPropertyDescriptor(source,e))}))}return n}var h={created:function(){try{var n=l.a.load("- name: Spell\n  worksWellWith:\n    - [Jupyter Notebooks]\n    - [Kubernetes]\n    - [Grafana]\n    - [Weights and Biases]\n    - [Arize]\n  buttonText: Try Spell\n  link: 'https://spell.ml/pricing'\n  category: Training Orchestration\n  description: >-\n    Spell is an **end-to-end deep learning platform** that automates complex ML infrastructure and operational work required to train and deploy AI models. Spell is fully hybrid-cloud, and can deploy easily into any cloud or on-prem hardware.\n\n\n    - **Run Orchestration:** Automate cloud training execution from a user's local CLI as a tracked and reproducible experiment, capturing all outputs and comprehensive metrics.\n\n    - **Model Serving:** Serve models directly into production from a model registry, complete with lineage metadata, backed by a managed Kubernetes cluster for maximum scalability and robustness.\n\n    - **Experiment Management:** Manage, organize, collaborate on, and visualize your entire ML training portfolio in the cloud, under one centralized control pane.\n  logo: /images/projects/spell.svg\n  introLogoVisible: true\n  youTubeVideoId: s2E5sfmEbec\n- name: Katonic.ai\n  worksWellWith:\n    - [Jupyter Notebooks]\n    - [Cloud]\n    - [Python]\n    - [R-Studio]\n    - [Tensorflow]\n    - [Spark]\n  buttonText: Try Katonic.ai\n  link: 'https://katonic.ai'\n  category: Training Orchestration\n  description: >-\n    Katonic MLOps Platform is a collaborative platform with a Unified UI to manage all data science activities in one place and introduce MLOps practice\n    into the production systems of customers and developers. It is a collection of cloud-native tools for all of these stages of MLOps:\n\n    - Data exploration\n\n    - Feature preparation\n\n    - Model training/tuning\n\n    - Model serving, testing and versioning\n\n\n    Katonic is for both data scientists and data engineers looking to build production-grade machine learning implementations and can be\n    run either locally in your development environment or on a production cluster.\n    Katonic provides a unified system—leveraging Kubernetes for containerization and scalability for the portability and repeatability of\n    its pipelines.\n\n  logo: /images/projects/katonic.png\n  introLogoVisible: true\n  youTubeVideoId: cGl5CqSiPLw\n- name: Orchest\n  worksWellWith:\n    - [Jupyter Notebooks]\n    - [Self-Hosted]\n    - [Cloud]\n  buttonText: Try Orchest\n  link: 'https://orchest.io'\n  category: Training Orchestration\n  description: >-\n    Build data pipelines, the easy way!\n\n\n    No framework. No YAML. Just write Python and R code in Notebooks.\n\n\n    Features:\n\n    - **Visually construct** pipelines through our user-friendly UI\n\n    - **Code in Notebooks**\n\n    - Run any subset of a pipeline **directly or periodically**\n\n    - Easily define your dependencies to run on **any machine**\n\n  logo: /images/projects/orchest.png\n  introLogoVisible: true\n  gitHubRepoName: orchest/orchest\n  youTubeVideoId: BUpSVE2mtz4\n- name: WhyLabs\n  buttonText: Try WhyLabs for Free\n  link: 'https://whylabs.ai/'\n  category: Model Monitoring\n  description: >-\n    The WhyLabs Observability Platform enables any AI practitioner to set up AI monitoring in three easy steps. It follows the standard DevOps model of installing a lightweight logging agent (whylogs) alongside your model and sending data profiles to a fully self-service SaaS platform (WhyLabs). On the platform, you can analyze your profiles to see how your model is performing and get automatically get alerted on changes.\n    The platform includes:\n\n    - **An easy setup flow** so that you can start getting value right away\n\n    - **Automatic data drift detection and alerting** to prevent model performance degradation\n\n    - **Industry standard for data profiling** enabled by the open source \"whylogs\" library\n\n  logo: /images/projects/whylabs.png\n  introLogoVisible: true\n  gitHubRepoName: whylabs/whylogs\n  youTubeVideoId: UsDsLEpigBw\n- name: Hypervector\n  buttonText: Try Hypervector\n  link: 'https://hypervector.io'\n  category: Model Testing\n  worksWellWith:\n    - [Github Actions]\n  description: >-\n    Provides API-powered **synthetic data test fixtures** for your tabular data-enabled features\n    — enabling regression and integration tests to be easily built and deployed for your\n    production ML models and data science components\n\n    - **Construct synthetic data** covering important behaviours, edge cases, and\n    under-represented areas of your input domain\n\n    - **Retrieve via dedicated endpoint** to your builds & continuous integration steps. No more data in version control.\n\n    - **Benchmark model output** to quickly detect regressions, or compare behaviours for model selection\n\n  logo: /images/projects/hypervector.png\n  introLogoVisible: true\n  gitHubRepoName: hypervectorio/hypervector-wrapper\n  youTubeVideoId: 877h1umtJAo\n- name: Syndicai\n  buttonText: Try Syndicai\n  link: 'https://syndicai.co'\n  category: Model Serving\n  worksWellWith:\n  - [Kubernetes]\n  - [Grafana]\n  - [Nvidia Triton]\n  - [Intel OpenVINO]\n  - [MLFLow]\n  - [ClearML]\n  description: >-\n    Syndicai is a cloud platform that deploys, manages, and scales any trained AI\n    model in minutes with no configuration & infrastructure setup.\n\n    - **Easy to use** - You don't need to know to understand Docker & Kubernetes.\n    Platform Production-ready deployments from day one.\n\n    - **Highly flexible** - Customize every single step of the whole AI model\n    deployment workflow (from model wrappers to Kubernetes configuration manifests),\n    and integrate with tools you love.\n\n    - **Optimized for ML** - Run workload on highly optimized, cost-efficient,\n    and secure infrastructure built specifically for high-performance ML models.\n\n    - **Cloud, Framework agnostic** - Deploy ML models written in frameworks you\n    love and run them on the cloud you want with no extensive and time-consuming setup.\n  logo: /images/projects/syndicai.svg\n  introLogoVisible: true\n- name: PrimeHub\n  buttonText: Try PrimeHub\n  link: https://github.com/InfuseAI/primehub\n  category: Training Orchestration\n  description: >-\n    PrimeHub, an open-source pluggable MLOps platform on the top of Kubernetes for teams of data scientists and administrators.\n    PrimeHub equips enterprises with consistent yet flexible tools to develop, train, and deploy ML models at scale.\n    By improving the iterative process of data science, data teams can collaborate closely and innovate fast.\n\n    - Cluster Computing with multi-tenancy\n\n    - One-Click Notebook Environments\n\n    - Group-centric Datasets Management / Resources Management / Access-control Management\n\n    - Custom Machine Learning Environments with Image Builder\n\n    - Model Tracking and Deployment\n\n    - Capability Augmentation with 3rd-party Apps Store\n\n  logo: /images/projects/primehub.png\n  introLogoVisible: true\n  gitHubRepoName: InfuseAI/primehub\n  youTubeVideoId: 3ZOPXR9L2Ho\n- name: DAGsHub\n  buttonText: Try DAGsHub\n  link: 'https://dagshub.com/'\n  category: Data Versioning\n  description: >-\n    DAGsHub enables data scientists and ML engineers to work together, effectively.\n    Integrating open-source tools like Git, DVC, MLflow, and Jenkins so that you can\n    track and version code, data, models, pipelines, and experiments in one place.\n\n    - **Your project in one place**: Manage your code, notebooks, data, models, pipelines, and\n    experiments and easily connect to plugins for automation, all with open source tools and open formats.\n\n    - **Zero configuration**: Don't waste time on DevOps heavy lifting. Each DAGsHub project comes with a free,\n    built-in DVC data storage and MLflow server, with team access controls, so you can just add the URL, and get to work.\n\n    - **Diff, compare, and review anything**: DAGsHub lets you diff Jupyter notebooks, tables, images, experiments, and even MRI data,\n    so you can compare apples to apples, review, and make sense of your work.\n\n    - **Reproducibility is a click away**: Get all components of an experiment on your system. It's as easy as `git checkout`.\n  logo: /images/projects/dagshub.svg\n  introLogoVisible: true\n  youTubeVideoId: lrzdqEwzoo8\n- name: Determined\n  buttonText: Try Determined\n  link: 'https://determined.ai/'\n  category: Training Orchestration\n  description: >-\n    Determined is an open-source deep learning training platform that makes\n    building models fast and easy.\n\n\n    - Train models faster using state-of-the-art distributed training, without\n    changing your model code\n\n    - Automatically find high-quality models with advanced hyperparameter tuning\n    from the creators of Hyperband\n\n    - Get more from your GPUs with smart scheduling and cut cloud GPU costs by\n    seamlessly using preemptible instances\n\n    - Track and reproduce your work with experiment tracking that works\n    out-of-the-box, covering code versions, metrics, checkpoints, and\n    hyperparameters\n\n\n    Determined integrates these features into an easy-to-use, high-performance\n    deep learning environment — which means you can spend your time building\n    models instead of managing infrastructure.\n  logo: /images/projects/determined.svg\n  introLogoVisible: true\n  gitHubRepoName: determined-ai/determined\n- name: Flyte\n  buttonText: Try Flyte\n  link: 'https://flyte.org/'\n  category: Training Orchestration\n  description: >-\n    Flyte makes it easy to create concurrent, scalable, and maintainable\n    workflows for machine learning and data processing.\n\n    - Kubernetes-Native Workflow Automation Platform\n\n    - Ergonomic SDK's in Python, Java & Scala\n\n    - Versioned & Auditable\n\n    - Reproducible Pipelines\n\n    - Strong Data Typing\n  logo: /images/projects/flyte.svg\n  introLogoVisible: true\n  gitHubRepoName: flyteorg/flyte\n  youTubeVideoId: 1BjXC5TZAiI\n- name: Kubeflow\n  buttonText: Try Kubeflow\n  link: 'https://www.kubeflow.org/'\n  category: Training Orchestration\n  description: >-\n    The Kubeflow project is dedicated to making deployments of machine learning\n    (ML) workflows on Kubernetes simple, portable and scalable.\n\n\n    Kubeflow's goal is not to recreate other services, but to provide a straightforward way to deploy\n    best-of-breed open-source systems for ML to diverse infrastructures.\n\n\n    Anywhere you are running Kubernetes, you should be able to run Kubeflow.\n  logo: /images/projects/kubeflow.svg\n  introLogoVisible: true\n  gitHubRepoName: kubeflow/kubeflow\n  youTubeVideoId: cTZArDgbIWw\n- name: OpenPAI\n  buttonText: Try OpenPAI\n  link: 'https://openpai.readthedocs.io/'\n  category: Training Orchestration\n  description: >-\n    OpenPAI is an open-source platform that provides complete AI model training\n    and resource management capabilities, it is easy to extend and supports\n    on-premise, cloud, and hybrid environments on various scales.\n  logo: /images/projects/pai.svg\n  introLogoVisible: true\n  gitHubRepoName: Microsoft/pai\n  youTubeVideoId: C9eRaqqAOeY\n- name: Butterfree\n  buttonText: Try Butterfree\n  link: 'https://github.com/quintoandar/butterfree'\n  category: Feature Store\n  description: >-\n    A tool for building feature stores. Transform your raw data into beautiful\n    features.\n\n\n    The library is centered on the following concetps:\n\n    - ETL: central framework to create data pipelines. Spark-based Extract, Transform and Load modules ready to use.\n\n    - Declarative Feature Engineering: care about what you want to compute and not how to code it.\n\n    - Feature Store Modeling: the library easily provides everything you need to process and load data to your Feature Store.\n\n  logo: /images/projects/quintoandar.svg\n  introLogoVisible: true\n  gitHubRepoName: quintoandar/butterfree\n- name: Feast\n  buttonText: Try Feast\n  link: 'https://feast.dev/'\n  category: Feature Store\n  competesWith:\n    - [Hopsworks]\n  description: >-\n    Feast is an operational data system for managing and serving machine learning features to models in production.\n\n    - Feast decouples your models from your data infrastructure by providing a single data access layer that abstracts feature storage from feature retrieval.\n\n    - Feast provides both a centralized registry to which data scientists can publish features, and a battle-hardened serving layer. Together, these enable non-engineering teams to ship features into production with minimal oversight.\n\n    - Feast solves the challenge of data leakage by providing point-in-time correct feature retrieval when exporting feature datasets for model training.\n\n    - With Feast, data scientists can start new ML projects by selecting previously engineered features from a centralized registry, and are no longer required to develop new features for each project.\n  logo: /images/projects/feast.svg\n  introLogoVisible: true\n  gitHubRepoName: feast-dev/feast\n  youTubeVideoId: DaNv-Wf1MBA\n- name: Hopsworks\n  buttonText: Try Hopsworks\n  link: 'https://www.hopsworks.ai/'\n  category: Feature Store\n  competesWith:\n    - [Tecton]\n    - [Feast]\n  description: >-\n    The Hopsworks Feature Store manages your features for training and serving models.\n\n    - Provides scale-out storage for training and batch inference as well as low-latency storage for online applications that need to build feature vectors to make real-time predictions.\n\n    - Provides Python and Java/Scala APIs to enable Batch and Online applications manage and use features for machine learning.\n\n    - Integrates seamlessly with popular platforms for Data Science, such as AWS Sagemaker and Databricks. It also integrates with backend datalakes, such as S3 and Hadoop.\n\n    - Supports both cloud and on-prem deployments.\n  logo: /images/projects/hopsworks.svg\n  introLogoVisible: true\n  gitHubRepoName: logicalclocks/hopsworks\n  youTubeVideoId: uVBeoeENEcI\n- name: MLFlow\n  buttonText: Try MLFlow\n  link: 'https://mlflow.org/'\n  category: Experiment Tracking\n  description: >-\n    MLflow is a platform to streamline machine learning development, including\n    tracking experiments, packaging code into reproducible runs, and sharing and\n    deploying models.\n\n\n    It offers a set of lightweight APIs that can be used with any existing machine learning application or library (TensorFlow, PyTorch, XGBoost, etc),\n    wherever you currently run ML code (e.g your notebook)\n\n\n    Features:\n\n\n    - **MLflow Tracking:** An API to log parameters, code, and results in machine learning experiments and compare them using an interactive UI.\n\n    - **MLflow Projects:** A code packaging format for reproducible runs using Conda and Docker, so you can share your ML code with others.\n\n    - **MLflow Models:** A model packaging format and tools that let you easily deploy the same model (from any ML library) to batch and real-time scoring on platforms such as Docker, Apache Spark, Azure ML and AWS SageMaker.\n\n    - **MLflow Model Registry:** A centralized model store, set of APIs, and UI, to collaboratively manage the full lifecycle of MLflow Models.\n  logo: /images/projects/mlflow.svg\n  introLogoVisible: true\n  gitHubRepoName: mlflow/mlflow\n  youTubeVideoId: VokAGy8C6K4\n- name: DVC\n  buttonText: Try DVC\n  link: 'https://dvc.org/'\n  category: Data Versioning\n  description: >-\n    DVC is an open-source tool for data science and machine learning projects.\n\n\n    Key features:\n\n\n    1. Simple command line Git-like experience. Does not require installing and\n    maintaining any databases. Does not depend on any proprietary online\n    services.\n\n    2. Management and versioning of datasets and machine learning models. Data\n    is saved in S3, Google cloud, Azure, Alibaba cloud, SSH server, HDFS, or\n    even local HDD RAID.\n\n    3. Makes projects reproducible and shareable; helping to answer questions\n    about how a model was built.\n\n    4. Helps manage experiments with Git tags/branches and metrics tracking.\n\n\n    DVC aims to replace spreadsheet and document sharing tools (such as Excel or\n    Google Docs) which are being used frequently as both knowledge repositories\n    and team ledgers. DVC also replaces both ad-hoc scripts to track, move, and\n    deploy different model versions; as well as ad-hoc data file suffixes and\n    prefixes.\n  logo: /images/projects/dvc.svg\n  introLogoVisible: true\n  gitHubRepoName: iterative/dvc\n- name: Pachyderm\n  buttonText: Try Pachyderm\n  link: 'https://www.pachyderm.com/'\n  category: Data Versioning\n  description: >-\n    Pachyderm is a tool for version-controlled, automated, end-to-end data\n    pipelines for data science.\n\n\n    Features:\n\n\n    - **Containerized**: Pachyderm is built on Docker and Kubernetes. Whatever languages or libraries your pipeline needs, they can run on Pachyderm which can easily be deployed on any cloud provider or on prem.\n\n    - **Version Control**: Pachyderm version controls your data as it's processed. You can always ask the system how data has changed, see a diff, and, if something doesn't look right, revert.\n\n    - **Provenance** (aka data lineage): Pachyderm tracks where data comes from. Pachyderm keeps track of all the code and data that created a result.\n\n    - **Parallelization**: Pachyderm can efficiently schedule massively parallel workloads.\n\n    - **Incremental Processing**: Pachyderm understands how your data has changed and is smart enough to only process the new data.\n  logo: /images/projects/pachyderm.svg\n  introLogoVisible: true\n  gitHubRepoName: pachyderm/pachyderm\n  youTubeVideoId: r9CrtAtuJDI\n- name: Weights & Biases\n  buttonText: Try Weights & Biases\n  link: 'https://wandb.ai/'\n  category: Experiment Tracking\n  description: >-\n    Track and visualize all the pieces of your machine learning pipeline, from datasets to production models.\n\n\n    - Quickly identify model regressions. Use W&B to visualize results in real time, all in a central dashboard.\n\n    - Focus on the interesting ML. Spend less time manually tracking results in spreadsheets and text files.\n\n    - Capture dataset versions with W&B Artifacts to identify how changing data affects your resulting models.\n\n    - Reproduce any model, with saved code, hyperparameters, launch commands, input data, and resulting model weights.\n  logo: /images/projects/wandb.png\n  introLogoVisible: true\n  gitHubRepoName: wandb/client\n  youTubeVideoId: 91HhNtmb0B4\n- name: Aim\n  buttonText: Try Aim\n  link: 'https://aimstack.io/'\n  category: Experiment Tracking\n  description: >-\n    Compare 1000s of AI experiments at once.\n\n\n    - **Open-source**: Community-driven. Self-hosted and full metadata access.\n\n    - **Explore & Compare**: Easily search, group and aggregate metrics by any hyperparameter.\n\n    - **Dashboard**: Activity view and full experiments dashboard for all experiments.\n  logo: /images/projects/aim.png\n  introLogoVisible: true\n  gitHubRepoName: aimhubio/aim\n- name: ClearML\n  buttonText: Try ClearML\n  link: 'https://clear.ml/'\n  category: Experiment Tracking\n  description: >-\n    ClearML is an open source suite of tools that automates preparing, executing, and\n    analyzing machine learning experiments.\n\n\n    Features:\n\n\n    - **ClearML Experiment:** A complete experiment management toolset. Keep track of parameters, jobs, artifacts, metrics, debug data, metadata, and log it all in one clear interface.\n\n    - **ClearML Orchestrate:** The easiest way to manage scheduling and orchestration for GPU / CPU resources and to auto-scale on cloud & on-prem machines. Replicate your dev environment for training anywhere or develop on remote VMs.\n\n    - **ClearML Feature Store:** Data analysis versioning & lineage for full reproducibility. Build and automate data pipelines for R&D and production. Rebalance, debias and mix & match datasets for fine grain control of your data.\n  logo: /images/projects/clearml.svg\n  introLogoVisible: true\n  gitHubRepoName: allegroai/clearml\n  youTubeVideoId: Y5tPfUm9Ghg\n- name: CML\n  buttonText: Try CML\n  link: 'http://cml.dev/'\n  category: Experiment Tracking\n  description: >-\n    Open-source library for implementing CI/CD in machine learning projects.\n\n\n    On every pull request, CML helps you automatically train and evaluate models, then generates a visual report with results and metrics.\n\n    - **GitFlow for data science.** Use GitLab or GitHub to manage ML experiments, track who trained ML models or modified data and when. Codify data and models with DVC instead of pushing to a Git repo.\n\n    - **Auto reports for ML experiments.** Auto-generate reports with metrics and plots in each Git Pull Request. Rigorous engineering practices help your team make informed, data-driven decisions.\n\n    - **No additional services.** Build your own ML platform using just GitHub or GitLab and your favourite cloud services: AWS, Azure, GCP. No databases, services or complex setup needed.\n  logo: /images/projects/cml.png\n  introLogoVisible: true\n  gitHubRepoName: iterative/cml\n  youTubeVideoId: 9BgIDqAzfuA\n- name: BentoML\n  buttonText: Try BentoML\n  link: 'https://bentoml.org/'\n  category: Model Serving\n  worksWellWith:\n    - [AWS Lambda]\n    - [Azure Functions]\n    - [Google Cloud Run]\n  description: >2-\n    BentoML is a flexible, high-performance framework for serving, managing, and deploying machine learning models.\n\n    - Supports multiple ML frameworks, including Tensorflow, PyTorch, Keras,\n    XGBoost and more\n\n    - Cloud native deployment with Docker, Kubernetes, AWS, Azure and many more\n\n    - High-Performance online API serving and offline batch serving\n\n    - Web dashboards and APIs for model registry and deployment management\n\n\n    BentoML tries to bridge the gap between Data Science and DevOps. By providing a\n    standard interface for describing a prediction service, BentoML abstracts\n    away how to run model inference efficiently and how model serving workloads\n    can integrate with cloud infrastructures.\n  competesWith:\n    - [Seldon Core, to some extent]\n    - [KFServing, to some extent]\n  logo: /images/projects/bentoml.svg\n  introLogoVisible: true\n  gitHubRepoName: bentoml/BentoML\n- name: Cortex\n  buttonText: Try Cortex\n  link: 'https://cortex.dev/'\n  category: Model Serving\n  description: >-\n    Cortex makes it simple to deploy machine learning models in production.\n\n\n    **Deploy**\n\n    - Deploy TensorFlow, PyTorch, ONNX, scikit-learn, and other models.\n\n    - Define preprocessing and postprocessing steps in Python.\n\n    - Configure AP/Is as realtime or batch.\n\n    - Deploy multiple models per API.\n\n\n    **Manage**\n\n    - Monitor API performance and track predictions.\n\n    - Update APIs with no downtime.\n\n    - Stream logs from APIs.\n\n    - Perform A/B tests.\n\n\n    **Scale**\n\n    - Test locally, scale on your AWS account.\n\n    - Autoscale to handle production traffic.\n\n    - Reduce cost with spot instances.\n  logo: /images/projects/cortex.svg\n  introLogoVisible: true\n  gitHubRepoName: cortexlabs/cortex\n- name: KFServing\n  buttonText: Try KFServing\n  link: 'https://github.com/kubeflow/kfserving'\n  category: Model Serving\n  competesWith:\n    - [Seldon Core]\n    - [BentoML, to some extent]\n  description: >-\n    KFServing enables serverless inferencing on Kubernetes to solve production model serving use cases.\n\n\n    - Provides performant, high abstraction interfaces for common ML frameworks like TensorFlow, XGBoost, scikit-learn, PyTorch, and ONNX.\n\n    - Provides a Kubernetes Custom Resource Definition (CRD) for serving ML models.\n\n    - Encapsulate the complexity of autoscaling, networking, health checking, and server configuration to bring cutting edge serving features like GPU autoscaling, scale to zero, and canary rollouts to your ML deployments.\n\n    - Enable a simple, pluggable, and complete story for your production ML inference server by providing prediction, pre-processing, post-processing and explainability out of the box.\n\n  logo: /images/projects/kubeflow.svg\n  introLogoVisible: true\n  gitHubRepoName: kubeflow/kfserving\n  youTubeVideoId: lj_X2ND2BBI\n- name: Triton Inference Server\n  buttonText: Try Triton\n  link: 'https://developer.nvidia.com/nvidia-triton-inference-server'\n  category: Model Serving\n  worksWellWith:\n    - [Azure ML]\n    - [Google CAIP]\n  description: >-\n    Triton Inference Server simplifies the deployment of AI models at scale in production.\n\n\n    - Supports TensorFlow, TensorRT, PyTorch, ONNX Runtime, and custom framework backends.\n\n    - Triton runs models concurrently on GPUs to maximize utilization, supports CPU-based inferencing, and offers advanced features like model ensemble and streaming inferencing.\n\n    - Available as a Docker container, Triton integrates with Kubernetes for orchestration and scaling.\n\n    - Can be used with cloud AI platforms like Azure ML and Google CAIP.\n\n    - Triton exports Prometheus metrics for monitoring.\n  logo: /images/projects/nvidia.svg\n  introLogoVisible: true\n  gitHubRepoName: triton-inference-server/server\n  youTubeVideoId: 1DUqD3zMwB4\n  competesWith:\n    - [Tensorflow Serving]\n    - [TorchServe]\n- name: Seldon Core\n  buttonText: Try Seldon Core\n  link: 'https://www.seldon.io/tech/products/core/'\n  category: Model Serving\n  competesWith:\n    - [KFServing]\n    - [BentoML, to some extent]\n  description: >-\n    Seldon Core makes it easier and faster to deploy your machine learning models and experiments at scale on Kubernetes.\n\n\n    - **Runs anywhere**: Built on Kubernetes, runs on any cloud and on premises\n\n    - **Agnostic and independent**: Framework agnostic, supports top ML libraries, toolkits and languages\n\n    - **Runtime inference graphs**: Advanced deployments with experiments, ensembles and transformers\n\n\n    Seldon handles scaling to thousands of production machine learning models\n    and provides advanced machine learning capabilities out of the box including\n    Advanced Metrics, Request Logging, Explainers, Outlier Detectors, A/B Tests,\n    Canaries and more.\n  logo: /images/projects/seldoncore.svg\n  introLogoVisible: true\n  gitHubRepoName: SeldonIO/seldon-core\n  youTubeVideoId: Xildxp_CsmA\n- name: Tensorflow Serving\n  buttonText: Try Tensorflow Serving\n  link: https://github.com/tensorflow/serving\n  category: Model Serving\n  competesWith:\n    - [Triton Inference Server]\n  description: >-\n    TensorFlow Serving is a flexible, high-performance serving system for TF models, designed for production environments.\n\n\n    - Can serve multiple models, or multiple versions of the same model simultaneously.\n\n    - Exposes both gRPC as well as HTTP inference endpoints.\n\n    - Allows deployment of new model versions without changing any client code.\n\n    - Supports canarying new versions and A/B testing experimental models.\n\n    - Adds minimal latency to inference time due to efficient, low-overhead implementation.\n  logo: /images/projects/tensorflow.svg\n  introLogoVisible: true\n  gitHubRepoName: tensorflow/serving\n  youTubeVideoId: q_IkJcPyNl0\n- name: TorchServe\n  buttonText: Try TorchServe\n  link: 'https://pytorch.org/serve/'\n  category: Model Serving\n  competesWith:\n    - [Triton Inference Server]\n  description: >-\n    TorchServe is a flexible and easy to use tool for serving PyTorch models.\n\n\n    - With TorchServe, PyTorch users can bring their models to production quicker, without having to write custom code: on top of providing a low latency prediction API, TorchServe embeds default handlers for the most common applications such as object detection and text classification.\n\n    - TorchServe includes multi-model serving, model versioning for A/B testing, monitoring metrics, and RESTful endpoints for application integration.\n\n    - TorchServe supports any machine learning environment, including Amazon SageMaker, container services, and Amazon Elastic Compute Cloud (EC2).\n  logo: /images/projects/pytorch.svg\n  introLogoVisible: true\n  gitHubRepoName: pytorch/pytorch\n  youTubeVideoId: 9lMMCwVhPpo\n- name: Aporia\n  buttonText: Try Aporia\n  link: 'https://aporia.com/'\n  category: Model Monitoring\n  description: >-\n    With Aporia data scientists and ML engineers can easily build monitoring for their ML models running in production.\n\n\n    Features:\n\n    - **Build your own monitors**: Easily define monitoring logic.\n\n    - **Concept drift & Data integrity detections**: Built-in monitors and alerts for prediction drift, data drift, data integrity issues and more.\n\n    - **Runs on your VPC**: Natively supports on-prem and cloud deployments.\n\n    - **User-friendly & flexible**: A simple, intuitive dashboard for all your models in production.\n\n    - **Data segments**: Define & monitor slices of data based on selected features.\n  logo: /images/projects/aporia-logo.svg\n  introLogoVisible: true\n  youTubeVideoId: 9oyZDXrmWMA\n- name: Evidently AI\n  buttonText: Try Evidently AI\n  link: 'https://evidentlyai.com/'\n  category: Model Monitoring\n  description: >-\n    Evidently helps analyze machine learning models during validation or production monitoring. It generates interactive reports from pandas DataFramesor csv files.\n\n\n    Features:\n\n    - **Model Health:** Quickly visualize model performance and important metrics. Get a prioritized list of issues to debug.\n\n    - **Data Drift:** Compare recent data with the past. Learn which features changed and if key models drivers shifted. Visually explore and understand drift.\n\n    - **Target Drift:** Understand how model predictions and target change over time. If the ground truth is delayed, catch the model decay in advance.\n  logo: /images/projects/evidentlyai.ico\n  introLogoVisible: true\n  youTubeVideoId: 2suSzXlY_7Y\n- name: Deepchecks\n  buttonText: Try Deepchecks\n  link: 'https://deepchecks.com/'\n  category: Model Monitoring\n  description: >-\n    Validate and monitor your data and models during training, production and new version releases.\n\n\n    Features:\n\n\n    - ML Validation of training data and ML model\n\n    - Observability of ML in production\n\n    - Alerting about various issues in live ML systems\n\n    - Detecting Mismatches between research and production environments\n\n    - Quick Querying of problematic production data\n  logo: /images/projects/deepchecks.png\n  introLogoVisible: true\n- name: ELI5\n  buttonText: Try ELI5\n  link: 'https://github.com/TeamHG-Memex/eli5'\n  category: Explainability\n  description: >-\n    ELI5 is a Python library which allows to visualize and debug various Machine\n    Learning models using unified API. It has built-in support for several ML\n    frameworks and provides a way to explain black-box models.\n  logo: /images/projects/teamhg.svg\n  introLogoVisible: true\n  gitHubRepoName: TeamHG-Memex/eli5\n  youTubeVideoId: s-yT5Is1G1A\n- name: InterpretML\n  buttonText: Try InterpretML\n  link: 'https://interpret.ml/'\n  category: Explainability\n  description: >-\n    InterpretML is an open-source package that incorporates state-of-the-art\n    machine learning interpretability techniques under one roof. With this\n    package, you can train interpretable glassbox models and explain blackbox\n    systems. InterpretML helps you understand your model's global behavior, or\n    understand the reasons behind individual predictions.\n\n\n    Interpretability is essential for:\n\n\n    - Model debugging - Why did my model make this mistake?\n\n    - Feature Engineering - How can I improve my model?\n\n    - Detecting fairness issues - Does my model discriminate?\n\n    - Human-AI cooperation - How can I understand and trust the model's\n    decisions?\n\n    - Regulatory compliance - Does my model satisfy legal requirements?\n\n    - High-risk applications - Healthcare, finance, judicial, ...\n  logo: /images/projects/interpretml.svg\n  introLogoVisible: true\n  gitHubRepoName: interpretml/interpret\n  youTubeVideoId: WwBeKMQ0-I8\n- name: Lime\n  buttonText: Try Lime\n  link: 'https://github.com/marcotcr/lime'\n  category: Explainability\n  description: >-\n    This project is about explaining what machine learning classifiers (or\n    models) are doing. At the moment, we support explaining individual\n    predictions for text classifiers or classifiers that act on tables (numpy\n    arrays of numerical or categorical data) or images, with a package called\n    lime (short for local interpretable model-agnostic explanations). Lime is\n    based on the work presented in this paper (bibtex here for citation).\n  introLogoVisible: false\n  gitHubRepoName: marcotcr/lime\n  youTubeVideoId: hUnRCxnydCc\n- name: SHAP\n  buttonText: Try SHAP\n  link: 'https://github.com/slundberg/shap'\n  category: Explainability\n  description: >-\n    SHAP (SHapley Additive exPlanations) is a game theoretic approach to explain\n    the output of any machine learning model. It connects optimal credit\n    allocation with local explanations using the classic Shapley values from\n    game theory and their related extensions.\n  logo: /images/projects/shap.svg\n  introLogoVisible: true\n  gitHubRepoName: slundberg/shap\n- name: Fiddler\n  buttonText: Try Fiddler\n  link: 'https://www.fiddler.ai/'\n  category: Explainability\n  description: >-\n    Continuously monitor, explain, and analyze AI systems at scale. With actionable insights build trustworthy, fair, and responsible AI monitoring.\n\n\n    - Complex AI systems are inherently black boxes with minimal insight into their operation.\n\n    - Explainable AI or XAI makes these AI black boxes more like AI glass-boxes by enabling users to always understand the ‘why’ behind their decisions.\n\n    - Identify, address, and share performance gaps and biases quickly for AI validation and debugging\n  logo: /images/projects/fiddler.png\n  introLogoVisible: true\n  youTubeVideoId: gqnDqrIzabw\n- name: Comet\n  buttonText: Try Comet\n  link: 'https://www.comet.ml/'\n  category: Experiment Tracking\n  description: >-\n    Comet enables data scientists and teams to track, compare, explain and\n    optimize experiments and models across the model’s entire lifecycle. From\n    training to production.\n  logo: /images/projects/cometml.png\n  introLogoVisible: true\n  youTubeVideoId: cX5tx202PXM\n- name: Bytehub\n  buttonText: Try Bytehub\n  link: 'https://www.bytehub.ai/'\n  category: Feature Store\n  description: >-\n    An easy-to-use feature store.\n\n\n    The Bytehub Feature Store is designed to:\n\n\n    - Be simple to use, with a Pandas-like API;\n\n    - Require no complicated infrastructure, running on a local Python\n    installation or in a cloud environment;\n\n    - Be optimised towards timeseries operations, making it highly suited to\n    applications such as those in finance, energy, forecasting; and\n\n    - Support simple time/value data as well as complex structures, e.g.\n    dictionaries.\n\n\n    It is built on Dask to support large datasets and cluster compute\n    environments.\n  introLogoVisible: false\n  gitHubRepoName: bytehub-ai/bytehub\n  youTubeVideoId: ucAlzaJoqeU\n- name: Bodywork\n  buttonText: Try Bodywork\n  link: 'https://github.com/bodywork-ml/bodywork-core/'\n  category: Model Serving\n  description: >-\n    Bodywork deploys machine learning projects developed in Python, to\n    Kubernetes. It helps you:\n\n\n    - serve models as microservices\n\n    - execute batch jobs\n\n    - run reproducible pipelines\n\n\n    On demand, or on a schedule. It automates repetitive DevOps tasks and frees\n    machine learning engineers to focus on what they do best - solving data\n    problems with machine learning.\n  logo: /images/projects/bodywork.png\n  introLogoVisible: true\n  gitHubRepoName: bodywork-ml/bodywork-core/\n- name: Neptune\n  buttonText: Try Neptune\n  link: 'https://neptune.ai/'\n  category: Experiment Tracking\n  description: >-\n    Neptune is a lightweight experiment logging/tracking tool that helps you\n    with your machine learning experiments.\n\n\n    Features:\n\n    - Rich experiment logging and tracking capabilities\n\n    - Python and R clients\n\n    - Experiments dashboards, views and comparison features\n\n    - Team management\n\n    - 25+ integrations with popular data science stack libraries\n\n    - Fast, reliable UI\n  logo: /images/projects/neptune.svg\n  introLogoVisible: true\n  gitHubRepoName: neptune-ai/neptune-client\n  youTubeVideoId: w9S5srkfSI4\n- name: Iguazio Data Science Platform\n  buttonText: Try Iguazio\n  link: 'https://www.iguazio.com/'\n  category: Feature Store\n  description: >-\n    The Iguazio Data Science Platform accelerates and scales development, deployment and management of your AI\n    applications with MLOps and end-to-end automation of machine learning pipelines. The platform includes an\n    online and offline feature store, fully integrated with automated model monitoring and drift detection, model serving\n    and dynamic scaling capabilities, all packaged in an open and managed platform.\n\n    - **Ingest Data from Any Source and Build Reusable Online and Offline Features:** Ingest and unify unstructured\n    and structured data in real-time and create online and offline features using Iguazio’s **Integrated Feature Store**.\n\n    - **Continuously Train and Evaluate Models at Scale:** Run experimentation over scalable serverless ML/DL runtimes\n    with automated tracking, data versioning, and continuous integration/delivery (CI/CD) support.\n\n    - **Deploy Models to Production in Seconds:** Deploy models and APIs from a Jupyter notebook or IDE to production\n    in just a few clicks and continuously monitor model performance and mitigate model drift.\n\n    - **Monitor Your Models and Data on the Fly:** Manage, govern and monitor your models and real-time features\n    in production with a simple dashboard integrated with Iguazio’s Feature Store.\n  logo: /images/projects/iguazio.png\n  introLogoVisible: true\n  youTubeVideoId: BzQQ1X4LgcQ\n- name: MLRun\n  buttonText: Try MLRun\n  link: 'https://mlrun.org/'\n  category: Model Monitoring\n  description: >-\n    MLRun is an end-to-end open-source MLOps orchestration framework to manage and automate your entire analytics and machine\n    learning lifecycle, from data ingestion, through model development to full pipeline deployment. MLRun eases\n    the development of machine learning pipelines at scale and helps ML teams build a robust process for moving\n    from the research phase to fully operational production deployments.\n\n    - **Feature and Artifact Store:** Handles the ingestion, processing, metadata, and storage of data and features\n    across multiple repositories and technologies.\n\n    - **Elastic Serverless Runtimes:** Converts simple code to scalable and managed microservices with\n    workload-specific runtime engines (such as Kubernetes jobs, Nuclio, Dask, Spark, and Horovod).\n\n    - **ML Pipeline Automation:** Automates data preparation, model training and testing, deployment of real-time\n    production pipelines, and **end-to-end model and feature monitoring**.\n\n    - **Central Management:** Provides a unified portal for managing the entire MLOps workflow. The portal\n    includes a UI, a CLI, and an SDK, which are accessible from anywhere.\n\n  logo: /images/projects/mlrun.png\n  introLogoVisible: true\n  gitHubRepoName: mlrun/mlrun\n  youTubeVideoId: imiTr1aXRKU\n- name: Ploomber\n  category: Training Orchestration\n  worksWellWith:\n    - [Kubernetes]\n    - [AWS Batch]\n    - [Airflow]\n  buttonText: Try Ploomber\n  link: https://github.com/ploomber/ploomber\n  description: >-\n    Develop and test workflows locally, seamlessly execute them in a distributed environment.\n\n\n    Features:\n\n    - **Cloud-agnostic.** Runs in Kubernetes, AWS Batch, and Airflow.\n\n    - **Integrates with Jupyter.** Develop interactively, deploy to the cloud without code changes.\n\n    - **Incremental builds.** Speed up execution by skipping tasks whose source code has not changed.\n\n    - **Flexible.** Supports functions, scripts, notebooks, and SQL scripts as tasks.\n\n    - **Parallelization.** Automatically parallelize independent tasks.\n\n    - **Interactive console.** Helps you debug workflows quickly.\n\n  logo: /images/projects/ploomber.svg\n  introLogoVisible: true\n  gitHubRepoName: ploomber/ploomber\n  youTubeVideoId: XCgX1AszVF4\n- name: Sagify\n  buttonText: Try Sagify\n  link: 'https://www.sagifyml.com/'\n  category: Model Serving\n  description: >-\n    A command-line utility to train and deploy Machine Learning and Deep Learning models on AWS SageMaker in a few simple steps.\n\n\n    Key features:\n\n    1. **Turn on ML superpowers:** Train, tune and deploy hundreds of ML models by implementing just 2 functions\n\n    2. **Focus 100% on Machine Learning:** Manage your ML models from one place without dealing with low level engineering tasks\n\n    3. **100% reliable:** No more flaky ML pipelines. Sagify offers 100% reliable training and deployment on AWS.\n  logo: /images/projects/sagify.png\n  introLogoVisible: true\n  gitHubRepoName: Kenza-AI/sagify\n- name: Valohai\n  buttonText: Try Valohai\n  link: 'https://valohai.com/'\n  category: Training Orchestration\n  description: >-\n    Valohai is an MLOps platform that handles machine orchestration, automatic reproducibility and deployment.\n\n    - **Technology agnostic:** Valohai runs everything in Docker containers so that you can run almost anything on it.\n\n    - **Runs on any cloud:** Valohai natively supports Azure, AWS, GCP and OpenStack.\n\n    - **API, CLI, GUI and Jupyter integration:** Valohai integrates to almost any workflow through its many interfaces.\n\n    - **Managed service:** Seasoned DevOps engineers manage Valohai – so you don’t have to be one.\n  logo: /images/projects/valohai.svg\n  introLogoVisible: true\n  youTubeVideoId: jnrSd2nqCWg\n- name: Spock\n  category: Training Orchestration\n  competesWith:\n    - [Hydra]\n  buttonText: Try Spock\n  link: https://github.com/fidelity/spock\n  description: >-\n    spock is a framework that helps manage complex parameter configurations during research and development of Python applications. spock lets you focus on the code you need to write instead of re-implementing boilerplate code like creating ArgParsers, reading configuration files, implementing traceability etc. In short, spock configurations are defined by simple and familiar class-based structures. This allows spock to support inheritance, read from multiple markdown formats, and allow hierarchical configuration by composition.\n\n    Features:\n\n    - **Simple Declaration:** Type checked parameters are defined within a @spock decorated class. Supports required/optional and automatic defaults.\n\n    - **Easily Managed Parameter Groups:** Each class automatically generates its own object within a single namespace.\n\n    - **Parameter Inheritance:** Classes support inheritance allowing for complex configurations derived from a common base set of parameters.\n\n    - **Complex Types:** Nested Lists/Tuples, List/Tuples of Enum of @spock classes, List of repeated @spock classes\n\n    - **Multiple Configuration File Types:** Configurations are specified from YAML, TOML, or JSON files.\n\n    - **Hierarchical Configuration:** Compose from multiple configuration files via simple include statements.\n\n    - **Command-Line Overrides:** Quickly experiment by overriding a value with automatically generated command line arguments.\n\n    - **Immutable:** All classes are frozen preventing any misuse or accidental overwrites (to the extent they can be in Python).\n\n    - **Tractability and Reproducibility:** Save runtime parameter configuration to YAML, TOML, or JSON with a single chained command (with extra runtime info such as Git info, Python version, machine FQDN, etc). The saved markdown file can be used as the configuration input to reproduce prior runtime configurations.\n\n    - **S3 Addon:** Automatically detects s3:// URI(s) and handles loading and saving spock configuration files when an active boto3.Session is passed in (plus any additional S3Transfer configurations)\n\n  logo: /images/projects/spock.png\n  introLogoVisible: true\n  gitHubRepoName: fidelity/spock\n- name: Stoke\n  category: Training Orchestration\n  competesWith:\n    - [HuggingFace Accelerate]\n    - [PyTorch Lightning (Accelerate)]\n  buttonText: Try Stoke\n  link: https://github.com/fidelity/stoke\n  description: >-\n    stoke is a lightweight wrapper for PyTorch that provides a simple declarative API for context switching between devices (e.g. CPU, GPU), distributed modes, mixed-precision, and PyTorch extensions. This allows you to switch from local full-precision CPU to mixed-precision distributed multi-GPU with extensions (like optimizer state sharding) by simply changing a few declarative flags. Additionally, stoke exposes configuration settings for every underlying backend for those that want configurability and raw access to the underlying libraries. In short, stoke is the best of PyTorch Lightning Accelerators disconnected from the rest of PyTorch Lightning. Write whatever PyTorch code you want, but leave device and backend context switching to stoke.\n\n    Supports:\n\n    - **Devices:** CPU, GPU, multi-GPU\n\n    - **Distributed:** DDP, Horovod, deepspeed (via DDP)\n\n    - **Mixed-Precision:** AMP, Nvidia Apex, deepspeed (custom APEX like backend)\n\n    - **Extensions:** fairscale (Optimizer State Sharding and Sharded DDP), deepspeed (ZeRO Stage 0-3, etc.)\n\n  logo: /images/projects/stoke.png\n  introLogoVisible: true\n  gitHubRepoName: fidelity/stoke\n- name: lakeFS\n  category: Data Versioning\n  buttonText: Try lakeFS!\n  link: https://github.com/treeverse/lakeFS\n  description: >-\n    lakeFS is an open-source data lake management platform that transforms your object storage into a Git-like repository. lakeFS enables you to manage your data lake the way you manage your code. Run parallel pipelines for experimentation and CI/CD for your data.\n\n    Features:\n\n    - **Scalable:** Version control data at exabyte scale.\n\n    - **Flexible:** Run git operations like branch, commit, and  merge over your data in any storage service.\n\n    - **Develop Faster:** Zero copy branching for frictionless experimentation, easy collaboration.\n\n    - **Enable Clean Workflows:** Use pre-commit & merge hooks for CI/CD workflows.\n\n    - **Resilient:** Recover from data issues faster with revert capability.\n\n  logo: /images/projects/lakefs.svg\n  introLogoVisible: true\n  gitHubRepoName: treeverse/lakeFS\n  youTubeVideoId: xThorxDzmrw\n- name: OpenVINO™ Model Server\n  buttonText: Try OpenVINO™ Model Server\n  link: https://github.com/openvinotoolkit/model_server\n  category: Model Serving\n  competesWith:\n    - [Triton Inference Server]\n    - [Tensorflow Serving]\n    - [TorchServe]\n  description: >-\n    OpenVINO™ Model Server (OVMS) is a scalable, high-performance solution for serving machine learning models optimized for Intel® architectures.\n\n\n    - Simultanous serving of any model trained in a framework that is supported by OpenVINO\n\n    - The server implements gRPC and REST API framework with data serialization and deserialization using TensorFlow Serving API\n\n    - Uses OpenVINO™ as the inference execution provider\n\n    - Supports different file systems: local (e.g. NFS), Google Cloud Storage (GCS), Amazon S3, Minio or Azure Blob Storage\n\n  logo: /images/projects/ovms.png\n  introLogoVisible: true\n  gitHubRepoName: openvinotoolkit/model_server\n  youTubeVideoId: AfytPrAVdfc\n");this.setProjects(n)}catch(n){console.error(n)}this.octoAuth()},methods:m(m({},Object(r.c)({setProjects:"setProjects",setOctokit:"setOctokit"})),{},{octoAuth:function(){this.setOctokit(new d.a)}})},y=t(53),component=Object(y.a)(h,(function(){var n=this.$createElement;return(this._self._c||n)("Nuxt")}),[],!1,null,null,null);e.a=component.exports},206:function(n,e,t){t(207),n.exports=t(208)},246:function(n,e,t){var content=t(247);content.__esModule&&(content=content.default),"string"==typeof content&&(content=[[n.i,content,""]]),content.locals&&(n.exports=content.locals);(0,t(96).default)("890c9cc4",content,!0,{sourceMap:!1})},247:function(n,e,t){var o=t(95),r=t(248),l=t(249),d=o((function(i){return i[1]})),c=r(l);d.push([n.i,"/*! tailwindcss v2.1.2 | MIT License | https://tailwindcss.com*/\n\n/*! modern-normalize v1.0.0 | MIT License | https://github.com/sindresorhus/modern-normalize */\n\n/*\nDocument\n========\n*/\n\n/**\nUse a better box model (opinionated).\n*/\n\n*,\n*::before,\n*::after {\n  box-sizing: border-box;\n}\n\n/**\nUse a more readable tab size (opinionated).\n*/\n\n:root {\n  -moz-tab-size: 4;\n  -o-tab-size: 4;\n     tab-size: 4;\n}\n\n/**\n1. Correct the line height in all browsers.\n2. Prevent adjustments of font size after orientation changes in iOS.\n*/\n\nhtml {\n  line-height: 1.15; /* 1 */\n  -webkit-text-size-adjust: 100%; /* 2 */\n}\n\n/*\nSections\n========\n*/\n\n/**\nRemove the margin in all browsers.\n*/\n\nbody {\n  margin: 0;\n}\n\n/**\nImprove consistency of default fonts in all browsers. (https://github.com/sindresorhus/modern-normalize/issues/3)\n*/\n\nbody {\n  font-family:\n\t\tsystem-ui,\n\t\t-apple-system, /* Firefox supports this but not yet `system-ui` */\n\t\t'Segoe UI',\n\t\tRoboto,\n\t\tHelvetica,\n\t\tArial,\n\t\tsans-serif,\n\t\t'Apple Color Emoji',\n\t\t'Segoe UI Emoji';\n}\n\n/*\nGrouping content\n================\n*/\n\n/**\n1. Add the correct height in Firefox.\n2. Correct the inheritance of border color in Firefox. (https://bugzilla.mozilla.org/show_bug.cgi?id=190655)\n*/\n\nhr {\n  height: 0; /* 1 */\n  color: inherit; /* 2 */\n}\n\n/*\nText-level semantics\n====================\n*/\n\n/**\nAdd the correct text decoration in Chrome, Edge, and Safari.\n*/\n\nabbr[title] {\n  -webkit-text-decoration: underline dotted;\n          text-decoration: underline dotted;\n}\n\n/**\nAdd the correct font weight in Edge and Safari.\n*/\n\nb,\nstrong {\n  font-weight: bolder;\n}\n\n/**\n1. Improve consistency of default fonts in all browsers. (https://github.com/sindresorhus/modern-normalize/issues/3)\n2. Correct the odd 'em' font sizing in all browsers.\n*/\n\ncode,\nkbd,\nsamp,\npre {\n  font-family:\n\t\tui-monospace,\n\t\tSFMono-Regular,\n\t\tConsolas,\n\t\t'Liberation Mono',\n\t\tMenlo,\n\t\tmonospace; /* 1 */\n  font-size: 1em; /* 2 */\n}\n\n/**\nAdd the correct font size in all browsers.\n*/\n\nsmall {\n  font-size: 80%;\n}\n\n/**\nPrevent 'sub' and 'sup' elements from affecting the line height in all browsers.\n*/\n\nsub,\nsup {\n  font-size: 75%;\n  line-height: 0;\n  position: relative;\n  vertical-align: baseline;\n}\n\nsub {\n  bottom: -0.25em;\n}\n\nsup {\n  top: -0.5em;\n}\n\n/*\nTabular data\n============\n*/\n\n/**\n1. Remove text indentation from table contents in Chrome and Safari. (https://bugs.chromium.org/p/chromium/issues/detail?id=999088, https://bugs.webkit.org/show_bug.cgi?id=201297)\n2. Correct table border color inheritance in all Chrome and Safari. (https://bugs.chromium.org/p/chromium/issues/detail?id=935729, https://bugs.webkit.org/show_bug.cgi?id=195016)\n*/\n\ntable {\n  text-indent: 0; /* 1 */\n  border-color: inherit; /* 2 */\n}\n\n/*\nForms\n=====\n*/\n\n/**\n1. Change the font styles in all browsers.\n2. Remove the margin in Firefox and Safari.\n*/\n\nbutton,\ninput,\noptgroup,\nselect,\ntextarea {\n  font-family: inherit; /* 1 */\n  font-size: 100%; /* 1 */\n  line-height: 1.15; /* 1 */\n  margin: 0; /* 2 */\n}\n\n/**\nRemove the inheritance of text transform in Edge and Firefox.\n1. Remove the inheritance of text transform in Firefox.\n*/\n\nbutton,\nselect { /* 1 */\n  text-transform: none;\n}\n\n/**\nCorrect the inability to style clickable types in iOS and Safari.\n*/\n\nbutton,\n[type='button'] {\n  -webkit-appearance: button;\n}\n\n/**\nRemove the inner border and padding in Firefox.\n*/\n\n/**\nRestore the focus styles unset by the previous rule.\n*/\n\n/**\nRemove the additional ':invalid' styles in Firefox.\nSee: https://github.com/mozilla/gecko-dev/blob/2f9eacd9d3d995c937b4251a5557d95d494c9be1/layout/style/res/forms.css#L728-L737\n*/\n\n/**\nRemove the padding so developers are not caught out when they zero out 'fieldset' elements in all browsers.\n*/\n\nlegend {\n  padding: 0;\n}\n\n/**\nAdd the correct vertical alignment in Chrome and Firefox.\n*/\n\nprogress {\n  vertical-align: baseline;\n}\n\n/**\nCorrect the cursor style of increment and decrement buttons in Safari.\n*/\n\n/**\n1. Correct the odd appearance in Chrome and Safari.\n2. Correct the outline style in Safari.\n*/\n\n/**\nRemove the inner padding in Chrome and Safari on macOS.\n*/\n\n/**\n1. Correct the inability to style clickable types in iOS and Safari.\n2. Change font properties to 'inherit' in Safari.\n*/\n\n/*\nInteractive\n===========\n*/\n\n/*\nAdd the correct display in Chrome and Safari.\n*/\n\nsummary {\n  display: list-item;\n}\n\n/**\n * Manually forked from SUIT CSS Base: https://github.com/suitcss/base\n * A thin layer on top of normalize.css that provides a starting point more\n * suitable for web applications.\n */\n\n/**\n * Removes the default spacing and border for appropriate elements.\n */\n\nblockquote,\ndl,\ndd,\nh1,\nh2,\nh3,\nh4,\nh5,\nh6,\nhr,\nfigure,\np,\npre {\n  margin: 0;\n}\n\nbutton {\n  background-color: transparent;\n  background-image: none;\n}\n\n/**\n * Work around a Firefox/IE bug where the transparent `button` background\n * results in a loss of the default `button` focus styles.\n */\n\nbutton:focus {\n  outline: 1px dotted;\n  outline: 5px auto -webkit-focus-ring-color;\n}\n\nfieldset {\n  margin: 0;\n  padding: 0;\n}\n\nol,\nul {\n  list-style: none;\n  margin: 0;\n  padding: 0;\n}\n\n/**\n * Tailwind custom reset styles\n */\n\n/**\n * 1. Use the user's configured `sans` font-family (with Tailwind's default\n *    sans-serif font stack as a fallback) as a sane default.\n * 2. Use Tailwind's default \"normal\" line-height so the user isn't forced\n *    to override it to ensure consistency even when using the default theme.\n */\n\nhtml {\n  font-family: Inter, sans-serif; /* 1 */\n  line-height: 1.5; /* 2 */\n}\n\n/**\n * Inherit font-family and line-height from `html` so users can set them as\n * a class directly on the `html` element.\n */\n\nbody {\n  font-family: inherit;\n  line-height: inherit;\n}\n\n/**\n * 1. Prevent padding and border from affecting element width.\n *\n *    We used to set this in the html element and inherit from\n *    the parent element for everything else. This caused issues\n *    in shadow-dom-enhanced elements like <details> where the content\n *    is wrapped by a div with box-sizing set to `content-box`.\n *\n *    https://github.com/mozdevs/cssremedy/issues/4\n *\n *\n * 2. Allow adding a border to an element by just adding a border-width.\n *\n *    By default, the way the browser specifies that an element should have no\n *    border is by setting it's border-style to `none` in the user-agent\n *    stylesheet.\n *\n *    In order to easily add borders to elements by just setting the `border-width`\n *    property, we change the default border-style for all elements to `solid`, and\n *    use border-width to hide them instead. This way our `border` utilities only\n *    need to set the `border-width` property instead of the entire `border`\n *    shorthand, making our border utilities much more straightforward to compose.\n *\n *    https://github.com/tailwindcss/tailwindcss/pull/116\n */\n\n*,\n::before,\n::after {\n  box-sizing: border-box; /* 1 */\n  border-width: 0; /* 2 */\n  border-style: solid; /* 2 */\n  border-color: #e5e7eb; /* 2 */\n}\n\n/*\n * Ensure horizontal rules are visible by default\n */\n\nhr {\n  border-top-width: 1px;\n}\n\n/**\n * Undo the `border-style: none` reset that Normalize applies to images so that\n * our `border-{width}` utilities have the expected effect.\n *\n * The Normalize reset is unnecessary for us since we default the border-width\n * to 0 on all elements.\n *\n * https://github.com/tailwindcss/tailwindcss/issues/362\n */\n\nimg {\n  border-style: solid;\n}\n\ntextarea {\n  resize: vertical;\n}\n\ninput::-moz-placeholder, textarea::-moz-placeholder {\n  opacity: 1;\n  color: #9ca3af;\n}\n\ninput::placeholder,\ntextarea::placeholder {\n  opacity: 1;\n  color: #9ca3af;\n}\n\nbutton {\n  cursor: pointer;\n}\n\ntable {\n  border-collapse: collapse;\n}\n\nh1,\nh2,\nh3,\nh4,\nh5,\nh6 {\n  font-size: inherit;\n  font-weight: inherit;\n}\n\n/**\n * Reset links to optimize for opt-in styling instead of\n * opt-out.\n */\n\na {\n  color: inherit;\n  text-decoration: inherit;\n}\n\n/**\n * Reset form element properties that are easy to forget to\n * style explicitly so you don't inadvertently introduce\n * styles that deviate from your design system. These styles\n * supplement a partial reset that is already applied by\n * normalize.css.\n */\n\nbutton,\ninput,\noptgroup,\nselect,\ntextarea {\n  padding: 0;\n  line-height: inherit;\n  color: inherit;\n}\n\n/**\n * Use the configured 'mono' font family for elements that\n * are expected to be rendered with a monospace font, falling\n * back to the system monospace stack if there is no configured\n * 'mono' font family.\n */\n\npre,\ncode,\nkbd,\nsamp {\n  font-family: ui-monospace, SFMono-Regular, Menlo, Monaco, Consolas, \"Liberation Mono\", \"Courier New\", monospace;\n}\n\n/**\n * Make replaced elements `display: block` by default as that's\n * the behavior you want almost all of the time. Inspired by\n * CSS Remedy, with `svg` added as well.\n *\n * https://github.com/mozdevs/cssremedy/issues/14\n */\n\nimg,\nsvg,\nvideo,\ncanvas,\naudio,\niframe,\nembed,\nobject {\n  display: block;\n  vertical-align: middle;\n}\n\n/**\n * Constrain images and videos to the parent width and preserve\n * their intrinsic aspect ratio.\n *\n * https://github.com/mozdevs/cssremedy/issues/14\n */\n\nimg,\nvideo {\n  max-width: 100%;\n  height: auto;\n}\n\n@font-face{\n  font-family:\"Inter\";\n\n  src:url("+c+') format("truetype supports variations"),url('+c+') format("truetype-variations");\n\n  font-weight:100 900\n}\n\nbutton,input,textarea{\n  outline:none!important\n}\n\nhtml{\n  scroll-behavior:smooth\n}\n\n.container{\n  width:100%;\n}\n\n\n.aspect-w-16{\n  position:relative;\n  padding-bottom:calc(var(--tw-aspect-h) / var(--tw-aspect-w) * 100%);\n}\n\n\n.aspect-w-16 > *{\n  position:absolute;\n  height:100%;\n  width:100%;\n  top:0;\n  right:0;\n  bottom:0;\n  left:0;\n}\n\n.aspect-w-16{\n  --tw-aspect-w:16;\n}\n\n.aspect-h-9{\n  --tw-aspect-h:9;\n}\n\n.appearance-none{\n  -webkit-appearance:none;\n     -moz-appearance:none;\n          appearance:none;\n}\n\n.bg-gray-50{\n  --tw-bg-opacity:1;\n  background-color:rgba(249, 250, 251, var(--tw-bg-opacity));\n}\n\n.bg-gray-900{\n  --tw-bg-opacity:1;\n  background-color:rgba(17, 24, 39, var(--tw-bg-opacity));\n}\n\n.bg-purple-600{\n  --tw-bg-opacity:1;\n  background-color:rgba(79, 70, 229, var(--tw-bg-opacity));\n}\n\n.hover\\:bg-gray-50:hover{\n  --tw-bg-opacity:1;\n  background-color:rgba(249, 250, 251, var(--tw-bg-opacity));\n}\n\n.hover\\:bg-gray-100:hover{\n  --tw-bg-opacity:1;\n  background-color:rgba(243, 244, 246, var(--tw-bg-opacity));\n}\n\n.hover\\:bg-gray-800:hover{\n  --tw-bg-opacity:1;\n  background-color:rgba(31, 41, 55, var(--tw-bg-opacity));\n}\n\n.hover\\:bg-purple-700:hover{\n  --tw-bg-opacity:1;\n  background-color:rgba(67, 56, 202, var(--tw-bg-opacity));\n}\n\n.bg-gradient-to-r{\n  background-image:linear-gradient(to right, var(--tw-gradient-stops));\n}\n\n.bg-gradient-to-l{\n  background-image:linear-gradient(to left, var(--tw-gradient-stops));\n}\n\n.from-transparent{\n  --tw-gradient-from:transparent;\n  --tw-gradient-stops:var(--tw-gradient-from), var(--tw-gradient-to, rgba(0, 0, 0, 0));\n}\n\n.to-white{\n  --tw-gradient-to:#fff;\n}\n\n.border-gray-200{\n  --tw-border-opacity:1;\n  border-color:rgba(229, 231, 235, var(--tw-border-opacity));\n}\n\n.group:hover .group-hover\\:border-purple-300{\n  --tw-border-opacity:1;\n  border-color:rgba(165, 180, 252, var(--tw-border-opacity));\n}\n\n.focus\\:border-purple-300:focus{\n  --tw-border-opacity:1;\n  border-color:rgba(165, 180, 252, var(--tw-border-opacity));\n}\n\n.rounded{\n  border-radius:0.25rem;\n}\n\n.rounded-md{\n  border-radius:0.375rem;\n}\n\n.border-solid{\n  border-style:solid;\n}\n\n.border{\n  border-width:1px;\n}\n\n.border-b{\n  border-bottom-width:1px;\n}\n\n.block{\n  display:block;\n}\n\n.inline-block{\n  display:inline-block;\n}\n\n.inline{\n  display:inline;\n}\n\n.flex{\n  display:flex;\n}\n\n.inline-flex{\n  display:inline-flex;\n}\n\n.table{\n  display:table;\n}\n\n.hidden{\n  display:none;\n}\n\n.flex-col{\n  flex-direction:column;\n}\n\n.flex-wrap{\n  flex-wrap:wrap;\n}\n\n.flex-nowrap{\n  flex-wrap:nowrap;\n}\n\n.items-center{\n  align-items:center;\n}\n\n.justify-start{\n  justify-content:flex-start;\n}\n\n.justify-center{\n  justify-content:center;\n}\n\n.justify-between{\n  justify-content:space-between;\n}\n\n.justify-around{\n  justify-content:space-around;\n}\n\n.flex-1{\n  flex:1 1 0%;\n}\n\n.flex-shrink-0{\n  flex-shrink:0;\n}\n\n.font-medium{\n  font-weight:500;\n}\n\n.font-semibold{\n  font-weight:600;\n}\n\n.font-bold{\n  font-weight:700;\n}\n\n.h-2{\n  height:0.5rem;\n}\n\n.h-3{\n  height:0.75rem;\n}\n\n.h-4{\n  height:1rem;\n}\n\n.h-5{\n  height:1.25rem;\n}\n\n.h-6{\n  height:1.5rem;\n}\n\n.h-8{\n  height:2rem;\n}\n\n.h-10{\n  height:2.5rem;\n}\n\n.h-11{\n  height:2.75rem;\n}\n\n.h-16{\n  height:4rem;\n}\n\n.h-full{\n  height:100%;\n}\n\n.text-3xl{\n  font-size:36px;\n  line-height:1.6;\n  letter-spacing:-0.025em;\n}\n\n.text-2xl{\n  font-size:24px;\n  line-height:1.6;\n  letter-spacing:-0.025em;\n}\n\n.text-xl{\n  font-size:20px;\n  line-height:1.6;\n  letter-spacing:-0.025em;\n}\n\n.text-lg{\n  font-size:18px;\n  line-height:1.8;\n}\n\n.text-sm{\n  font-size:14px;\n  line-height:1.8;\n}\n\n.text-xs{\n  font-size:12px;\n  line-height:1.6;\n}\n\n.text-zero{\n  font-size:0px;\n}\n\n.m-1{\n  margin:0.25rem;\n}\n\n.-m-1{\n  margin:-0.25rem;\n}\n\n.my-1{\n  margin-top:0.25rem;\n  margin-bottom:0.25rem;\n}\n\n.mx-2{\n  margin-left:0.5rem;\n  margin-right:0.5rem;\n}\n\n.my-5{\n  margin-top:1.25rem;\n  margin-bottom:1.25rem;\n}\n\n.mx-auto{\n  margin-left:auto;\n  margin-right:auto;\n}\n\n.mt-0{\n  margin-top:0px;\n}\n\n.mt-1{\n  margin-top:0.25rem;\n}\n\n.mr-2{\n  margin-right:0.5rem;\n}\n\n.mb-2{\n  margin-bottom:0.5rem;\n}\n\n.ml-2{\n  margin-left:0.5rem;\n}\n\n.mt-3{\n  margin-top:0.75rem;\n}\n\n.mr-3{\n  margin-right:0.75rem;\n}\n\n.mb-3{\n  margin-bottom:0.75rem;\n}\n\n.ml-3{\n  margin-left:0.75rem;\n}\n\n.mt-4{\n  margin-top:1rem;\n}\n\n.mr-4{\n  margin-right:1rem;\n}\n\n.mb-4{\n  margin-bottom:1rem;\n}\n\n.ml-4{\n  margin-left:1rem;\n}\n\n.mt-5{\n  margin-top:1.25rem;\n}\n\n.mr-5{\n  margin-right:1.25rem;\n}\n\n.mt-6{\n  margin-top:1.5rem;\n}\n\n.mt-8{\n  margin-top:2rem;\n}\n\n.mt-10{\n  margin-top:2.5rem;\n}\n\n.mr-10{\n  margin-right:2.5rem;\n}\n\n.mb-10{\n  margin-bottom:2.5rem;\n}\n\n.ml-10{\n  margin-left:2.5rem;\n}\n\n.ml-20{\n  margin-left:5rem;\n}\n\n.ml-auto{\n  margin-left:auto;\n}\n\n.mt-0\\.5{\n  margin-top:0.125rem;\n}\n\n.mt-1\\.5{\n  margin-top:0.375rem;\n}\n\n.-mt-1{\n  margin-top:-0.25rem;\n}\n\n.-mb-6{\n  margin-bottom:-1.5rem;\n}\n\n.-mt-1\\.5{\n  margin-top:-0.375rem;\n}\n\n.max-w-lg{\n  max-width:32rem;\n}\n\n.max-w-screen{\n  max-width:160rem;\n}\n\n.object-cover{\n  -o-object-fit:cover;\n     object-fit:cover;\n}\n\n.opacity-0{\n  opacity:0;\n}\n\n.hover\\:opacity-70:hover{\n  opacity:0.7;\n}\n\n.overflow-hidden{\n  overflow:hidden;\n}\n\n.overflow-x-auto{\n  overflow-x:auto;\n}\n\n.p-1{\n  padding:0.25rem;\n}\n\n.p-10{\n  padding:2.5rem;\n}\n\n.py-1{\n  padding-top:0.25rem;\n  padding-bottom:0.25rem;\n}\n\n.py-2{\n  padding-top:0.5rem;\n  padding-bottom:0.5rem;\n}\n\n.px-2{\n  padding-left:0.5rem;\n  padding-right:0.5rem;\n}\n\n.py-3{\n  padding-top:0.75rem;\n  padding-bottom:0.75rem;\n}\n\n.px-3{\n  padding-left:0.75rem;\n  padding-right:0.75rem;\n}\n\n.px-4{\n  padding-left:1rem;\n  padding-right:1rem;\n}\n\n.py-5{\n  padding-top:1.25rem;\n  padding-bottom:1.25rem;\n}\n\n.px-6{\n  padding-left:1.5rem;\n  padding-right:1.5rem;\n}\n\n.py-10{\n  padding-top:2.5rem;\n  padding-bottom:2.5rem;\n}\n\n.px-10{\n  padding-left:2.5rem;\n  padding-right:2.5rem;\n}\n\n.px-2\\.5{\n  padding-left:0.625rem;\n  padding-right:0.625rem;\n}\n\n.pt-1{\n  padding-top:0.25rem;\n}\n\n.pt-2{\n  padding-top:0.5rem;\n}\n\n.pr-2{\n  padding-right:0.5rem;\n}\n\n.pb-6{\n  padding-bottom:1.5rem;\n}\n\n.pr-7{\n  padding-right:1.75rem;\n}\n\n.pt-9{\n  padding-top:2.25rem;\n}\n\n.pb-10{\n  padding-bottom:2.5rem;\n}\n\n.pr-12{\n  padding-right:3rem;\n}\n\n.pl-12{\n  padding-left:3rem;\n}\n\n.pt-24{\n  padding-top:6rem;\n}\n\n.placeholder-gray-400::-moz-placeholder{\n  --tw-placeholder-opacity:1;\n  color:rgba(156, 163, 175, var(--tw-placeholder-opacity));\n}\n\n.placeholder-gray-400::placeholder{\n  --tw-placeholder-opacity:1;\n  color:rgba(156, 163, 175, var(--tw-placeholder-opacity));\n}\n\n.focus\\:placeholder-gray-500:focus::-moz-placeholder{\n  --tw-placeholder-opacity:1;\n  color:rgba(107, 114, 128, var(--tw-placeholder-opacity));\n}\n\n.focus\\:placeholder-gray-500:focus::placeholder{\n  --tw-placeholder-opacity:1;\n  color:rgba(107, 114, 128, var(--tw-placeholder-opacity));\n}\n\n.pointer-events-none{\n  pointer-events:none;\n}\n\n.static{\n  position:static;\n}\n\n.absolute{\n  position:absolute;\n}\n\n.relative{\n  position:relative;\n}\n\n.sticky{\n  position:sticky;\n}\n\n.top-0{\n  top:0px;\n}\n\n.right-0{\n  right:0px;\n}\n\n.bottom-0{\n  bottom:0px;\n}\n\n.left-0{\n  left:0px;\n}\n\n.right-10{\n  right:2.5rem;\n}\n\n.left-10{\n  left:2.5rem;\n}\n\n.bottom-14{\n  bottom:3.5rem;\n}\n\n.top-24{\n  top:6rem;\n}\n\n.-right-24{\n  right:-6rem;\n}\n\n.-left-24{\n  left:-6rem;\n}\n\n.resize{\n  resize:both;\n}\n\n*{\n  --tw-shadow:0 0 #0000;\n}\n\n.shadow-sm{\n  --tw-shadow:0 1px 2px 0 rgba(0, 0, 0, 0.05);\n  box-shadow:var(--tw-ring-offset-shadow, 0 0 #0000), var(--tw-ring-shadow, 0 0 #0000), var(--tw-shadow);\n}\n\n*{\n  --tw-ring-inset:var(--tw-empty,/*!*/ /*!*/);\n  --tw-ring-offset-width:0px;\n  --tw-ring-offset-color:#fff;\n  --tw-ring-color:rgba(59, 130, 246, 0.5);\n  --tw-ring-offset-shadow:0 0 #0000;\n  --tw-ring-shadow:0 0 #0000;\n}\n\n.focus\\:ring:focus{\n  --tw-ring-offset-shadow:var(--tw-ring-inset) 0 0 0 var(--tw-ring-offset-width) var(--tw-ring-offset-color);\n  --tw-ring-shadow:var(--tw-ring-inset) 0 0 0 calc(3px + var(--tw-ring-offset-width)) var(--tw-ring-color);\n  box-shadow:var(--tw-ring-offset-shadow), var(--tw-ring-shadow), var(--tw-shadow, 0 0 #0000);\n}\n\n.focus\\:ring-purple-50:focus{\n  --tw-ring-opacity:1;\n  --tw-ring-color:rgba(238, 242, 255, var(--tw-ring-opacity));\n}\n\n.text-center{\n  text-align:center;\n}\n\n.text-right{\n  text-align:right;\n}\n\n.text-white{\n  --tw-text-opacity:1;\n  color:rgba(255, 255, 255, var(--tw-text-opacity));\n}\n\n.text-gray-400{\n  --tw-text-opacity:1;\n  color:rgba(156, 163, 175, var(--tw-text-opacity));\n}\n\n.text-gray-500{\n  --tw-text-opacity:1;\n  color:rgba(107, 114, 128, var(--tw-text-opacity));\n}\n\n.text-gray-600{\n  --tw-text-opacity:1;\n  color:rgba(75, 85, 99, var(--tw-text-opacity));\n}\n\n.text-gray-900{\n  --tw-text-opacity:1;\n  color:rgba(17, 24, 39, var(--tw-text-opacity));\n}\n\n.text-aporia{\n  --tw-text-opacity:1;\n  color:rgba(72, 207, 173, var(--tw-text-opacity));\n}\n\n.text-aporiaRed{\n  --tw-text-opacity:1;\n  color:rgba(242, 59, 39, var(--tw-text-opacity));\n}\n\n.group:hover .group-hover\\:text-purple-600{\n  --tw-text-opacity:1;\n  color:rgba(79, 70, 229, var(--tw-text-opacity));\n}\n\n.hover\\:text-purple-600:hover{\n  --tw-text-opacity:1;\n  color:rgba(79, 70, 229, var(--tw-text-opacity));\n}\n\n.hover\\:text-aporia:hover{\n  --tw-text-opacity:1;\n  color:rgba(72, 207, 173, var(--tw-text-opacity));\n}\n\n.align-top{\n  vertical-align:top;\n}\n\n.align-middle{\n  vertical-align:middle;\n}\n\n.whitespace-nowrap{\n  white-space:nowrap;\n}\n\n.w-2{\n  width:0.5rem;\n}\n\n.w-3{\n  width:0.75rem;\n}\n\n.w-4{\n  width:1rem;\n}\n\n.w-5{\n  width:1.25rem;\n}\n\n.w-6{\n  width:1.5rem;\n}\n\n.w-8{\n  width:2rem;\n}\n\n.w-10{\n  width:2.5rem;\n}\n\n.w-16{\n  width:4rem;\n}\n\n.w-7\\/12{\n  width:58.333333%;\n}\n\n.w-full{\n  width:100%;\n}\n\n.z-10{\n  z-index:10;\n}\n\n.transform{\n  --tw-translate-x:0;\n  --tw-translate-y:0;\n  --tw-rotate:0;\n  --tw-skew-x:0;\n  --tw-skew-y:0;\n  --tw-scale-x:1;\n  --tw-scale-y:1;\n  transform:translateX(var(--tw-translate-x)) translateY(var(--tw-translate-y)) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));\n}\n\n.transform-gpu{\n  --tw-translate-x:0;\n  --tw-translate-y:0;\n  --tw-rotate:0;\n  --tw-skew-x:0;\n  --tw-skew-y:0;\n  --tw-scale-x:1;\n  --tw-scale-y:1;\n  transform:translate3d(var(--tw-translate-x), var(--tw-translate-y), 0) rotate(var(--tw-rotate)) skewX(var(--tw-skew-x)) skewY(var(--tw-skew-y)) scaleX(var(--tw-scale-x)) scaleY(var(--tw-scale-y));\n}\n\n.rotate-180{\n  --tw-rotate:180deg;\n}\n\n.translate-x-4{\n  --tw-translate-x:1rem;\n}\n\n.translate-y-5{\n  --tw-translate-y:1.25rem;\n}\n\n.transition-all{\n  transition-property:all;\n  transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);\n  transition-duration:150ms;\n}\n\n.transition{\n  transition-property:background-color, border-color, color, fill, stroke, opacity, box-shadow, transform, filter, -webkit-backdrop-filter;\n  transition-property:background-color, border-color, color, fill, stroke, opacity, box-shadow, transform, filter, backdrop-filter;\n  transition-property:background-color, border-color, color, fill, stroke, opacity, box-shadow, transform, filter, backdrop-filter, -webkit-backdrop-filter;\n  transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);\n  transition-duration:150ms;\n}\n\n.transition-colors{\n  transition-property:background-color, border-color, color, fill, stroke;\n  transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);\n  transition-duration:150ms;\n}\n\n.transition-opacity{\n  transition-property:opacity;\n  transition-timing-function:cubic-bezier(0.4, 0, 0.2, 1);\n  transition-duration:150ms;\n}\n\n.duration-200{\n  transition-duration:200ms;\n}\n\n@-webkit-keyframes spin{\n  to{\n    transform:rotate(360deg);\n  }\n}\n\n@keyframes spin{\n  to{\n    transform:rotate(360deg);\n  }\n}\n\n@-webkit-keyframes ping{\n  75%, 100%{\n    transform:scale(2);\n    opacity:0;\n  }\n}\n\n@keyframes ping{\n  75%, 100%{\n    transform:scale(2);\n    opacity:0;\n  }\n}\n\n@-webkit-keyframes pulse{\n  50%{\n    opacity:.5;\n  }\n}\n\n@keyframes pulse{\n  50%{\n    opacity:.5;\n  }\n}\n\n@-webkit-keyframes bounce{\n  0%, 100%{\n    transform:translateY(-25%);\n    -webkit-animation-timing-function:cubic-bezier(0.8,0,1,1);\n            animation-timing-function:cubic-bezier(0.8,0,1,1);\n  }\n\n  50%{\n    transform:none;\n    -webkit-animation-timing-function:cubic-bezier(0,0,0.2,1);\n            animation-timing-function:cubic-bezier(0,0,0.2,1);\n  }\n}\n\n@keyframes bounce{\n  0%, 100%{\n    transform:translateY(-25%);\n    -webkit-animation-timing-function:cubic-bezier(0.8,0,1,1);\n            animation-timing-function:cubic-bezier(0.8,0,1,1);\n  }\n\n  50%{\n    transform:none;\n    -webkit-animation-timing-function:cubic-bezier(0,0,0.2,1);\n            animation-timing-function:cubic-bezier(0,0,0.2,1);\n  }\n}\n\n.filter{\n  --tw-blur:var(--tw-empty,/*!*/ /*!*/);\n  --tw-brightness:var(--tw-empty,/*!*/ /*!*/);\n  --tw-contrast:var(--tw-empty,/*!*/ /*!*/);\n  --tw-grayscale:var(--tw-empty,/*!*/ /*!*/);\n  --tw-hue-rotate:var(--tw-empty,/*!*/ /*!*/);\n  --tw-invert:var(--tw-empty,/*!*/ /*!*/);\n  --tw-saturate:var(--tw-empty,/*!*/ /*!*/);\n  --tw-sepia:var(--tw-empty,/*!*/ /*!*/);\n  --tw-drop-shadow:var(--tw-empty,/*!*/ /*!*/);\n  filter:var(--tw-blur) var(--tw-brightness) var(--tw-contrast) var(--tw-grayscale) var(--tw-hue-rotate) var(--tw-invert) var(--tw-saturate) var(--tw-sepia) var(--tw-drop-shadow);\n}\n\n.backdrop-filter{\n  --tw-backdrop-blur:var(--tw-empty,/*!*/ /*!*/);\n  --tw-backdrop-brightness:var(--tw-empty,/*!*/ /*!*/);\n  --tw-backdrop-contrast:var(--tw-empty,/*!*/ /*!*/);\n  --tw-backdrop-grayscale:var(--tw-empty,/*!*/ /*!*/);\n  --tw-backdrop-hue-rotate:var(--tw-empty,/*!*/ /*!*/);\n  --tw-backdrop-invert:var(--tw-empty,/*!*/ /*!*/);\n  --tw-backdrop-opacity:var(--tw-empty,/*!*/ /*!*/);\n  --tw-backdrop-saturate:var(--tw-empty,/*!*/ /*!*/);\n  --tw-backdrop-sepia:var(--tw-empty,/*!*/ /*!*/);\n  -webkit-backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia);\n          backdrop-filter:var(--tw-backdrop-blur) var(--tw-backdrop-brightness) var(--tw-backdrop-contrast) var(--tw-backdrop-grayscale) var(--tw-backdrop-hue-rotate) var(--tw-backdrop-invert) var(--tw-backdrop-opacity) var(--tw-backdrop-saturate) var(--tw-backdrop-sepia);\n}\n\n.backdrop-blur-xl{\n  --tw-backdrop-blur:blur(24px);\n}\n\n.column-count-3{\n  -moz-column-count:3;\n       column-count:3\n}\n\n.column-gap-10{\n  -moz-column-gap:2.5rem;\n       column-gap:2.5rem\n}\n\n@media (max-width: 1920px){\n  .\\32xl\\:column-count-2{\n    -moz-column-count:2;\n         column-count:2\n  }\n}\n\n@media (max-width: 1279px){\n}\n\n@media (max-width: 1023px){\n  .lg\\:block{\n    display:block;\n  }\n\n  .lg\\:hidden{\n    display:none;\n  }\n\n  .lg\\:flex-col{\n    flex-direction:column;\n  }\n\n  .lg\\:font-medium{\n    font-weight:500;\n  }\n\n  .lg\\:h-4{\n    height:1rem;\n  }\n\n  .lg\\:h-6{\n    height:1.5rem;\n  }\n\n  .lg\\:h-10{\n    height:2.5rem;\n  }\n\n  .lg\\:text-xl{\n    font-size:20px;\n    line-height:1.6;\n    letter-spacing:-0.025em;\n  }\n\n  .lg\\:text-base{\n    font-size:16px;\n    line-height:1.8;\n  }\n\n  .lg\\:text-xs{\n    font-size:12px;\n    line-height:1.6;\n  }\n\n  .lg\\:my-2{\n    margin-top:0.5rem;\n    margin-bottom:0.5rem;\n  }\n\n  .lg\\:mr-0{\n    margin-right:0px;\n  }\n\n  .lg\\:mt-1{\n    margin-top:0.25rem;\n  }\n\n  .lg\\:mt-2{\n    margin-top:0.5rem;\n  }\n\n  .lg\\:mr-2{\n    margin-right:0.5rem;\n  }\n\n  .lg\\:mr-4{\n    margin-right:1rem;\n  }\n\n  .lg\\:mt-5{\n    margin-top:1.25rem;\n  }\n\n  .lg\\:mt-6{\n    margin-top:1.5rem;\n  }\n\n  .lg\\:mb-6{\n    margin-bottom:1.5rem;\n  }\n\n  .lg\\:ml-6{\n    margin-left:1.5rem;\n  }\n\n  .lg\\:mt-8{\n    margin-top:2rem;\n  }\n\n  .lg\\:mt-10{\n    margin-top:2.5rem;\n  }\n\n  .lg\\:max-w-md{\n    max-width:28rem;\n  }\n\n  .lg\\:p-6{\n    padding:1.5rem;\n  }\n\n  .lg\\:py-2{\n    padding-top:0.5rem;\n    padding-bottom:0.5rem;\n  }\n\n  .lg\\:px-3{\n    padding-left:0.75rem;\n    padding-right:0.75rem;\n  }\n\n  .lg\\:py-4{\n    padding-top:1rem;\n    padding-bottom:1rem;\n  }\n\n  .lg\\:px-6{\n    padding-left:1.5rem;\n    padding-right:1.5rem;\n  }\n\n  .lg\\:pt-0{\n    padding-top:0px;\n  }\n\n  .lg\\:pb-2{\n    padding-bottom:0.5rem;\n  }\n\n  .lg\\:pt-6{\n    padding-top:1.5rem;\n  }\n\n  .lg\\:pb-10{\n    padding-bottom:2.5rem;\n  }\n\n  .lg\\:pt-16{\n    padding-top:4rem;\n  }\n\n  .lg\\:text-center{\n    text-align:center;\n  }\n\n  .lg\\:hover\\:text-gray-400:hover{\n    --tw-text-opacity:1;\n    color:rgba(156, 163, 175, var(--tw-text-opacity));\n  }\n\n  .lg\\:w-4{\n    width:1rem;\n  }\n\n  .lg\\:w-6{\n    width:1.5rem;\n  }\n\n  .lg\\:w-10{\n    width:2.5rem;\n  }\n\n  .lg\\:column-count-1{\n    -moz-column-count:1;\n         column-count:1\n  }\n}\n\n@media (max-width: 767px){\n}\n\n@media (max-width: 639px){\n}',""]),n.exports=d},249:function(n,e,t){n.exports=t.p+"fonts/Inter-VariableFont_slnt,wght.f958c68.ttf"},254:function(n,e,t){"use strict";t.r(e),t.d(e,"state",(function(){return d})),t.d(e,"getters",(function(){return c})),t.d(e,"mutations",(function(){return m}));var o=t(204),r=(t(11),t(255),t(27),t(259),t(261),t(263),t(264),t(265),t(266),t(267),t(268),t(269),t(270),t(271),t(272),t(273),t(274),t(275),t(276),t(31),t(43),t(33),t(24),t(131)),l=t.n(r),d=function(){return{showLogoInFilters:!1,projects:[],categoryColors:{"":["#374151","#f3f4f6"],"Model Serving":["#53a7ee","#e8eff6"],"Feature Store":["#39DAA3","#F0FBF7"],"Experiment Tracking":["#F09240","#FCEEE4"],"Model Monitoring":["#AF72FD","#E0D4F4"],"Model Testing":["#2b1048","#E0D4F4"],"Data Versioning":["#60DFE8","#DCF5F7"],"Training Orchestration":["#F23B27","#FFF4F5"],Explainability:["#F6E278","#FCF5E0"]},octokit:{}}},c={getProjects:function(n){return l.a.orderBy(n.projects,"name","asc")},getCategories:function(n){return Object(o.a)(new Set(n.projects.map((function(n){return n.category}))))},getProjectsIntroLogos:function(n){return l.a.orderBy(n.projects.filter((function(n){return n.introLogoVisible})).map((function(e){return{src:e.logo,name:e.name,class:e.introLogoClass,color:n.categoryColors[e.category][0]}})),"name","asc")},getCategoryColors:function(n){return n.categoryColors}},m={setProjects:function(n,e){n.projects=e},setShowLogoInFilters:function(n,e){n.showLogoInFilters=e},setOctokit:function(n,e){n.octokit=e}}},349:function(n,e,t){var map={"./icons.svg":350};function o(n){var e=r(n);return t(e)}function r(n){if(!t.o(map,n)){var e=new Error("Cannot find module '"+n+"'");throw e.code="MODULE_NOT_FOUND",e}return map[n]}o.keys=function(){return Object.keys(map)},o.resolve=r,n.exports=o,o.id=349},350:function(n,e,t){"use strict";t.r(e),e.default=t.p+"d6cc55b9c9ad776fce4732d9c5ad2dca.svg"}},[[206,15,3,16]]]);